{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":94635,"databundleVersionId":13121456,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":12910591,"sourceType":"datasetVersion","datasetId":8169059},{"sourceId":541597,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":418051,"modelId":435712}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Copy map-requirements to working directory\n!cp -r /kaggle/input/jigsaw-requirement/ /kaggle/working/jigsaw-requirement/","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Make install_requirements.sh script executable\n!chmod +x /kaggle/working/jigsaw-requirements/install_requirements.sh","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Run install_requirements.sh script\n!/kaggle/working/jigsaw-requirements/install_requirements.sh","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load train and test data\ntrain_df = pd.read_csv('/kaggle/input/jigsaw-agile-community-rules/train.csv')\ntest_df = pd.read_csv('/kaggle/input/jigsaw-agile-community-rules/test.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Preprocess text columns\ndef preprocessed_text(text):\n    import re\n    text = str(text)\n    text = re.sub(r'http\\S+', '', text)\n    text = re.sub(r'[^\\w\\s.,!?]', '', text)\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text\nfor col in ['body', 'positive_example_1', 'positive_example_2', 'negative_example_1', 'negative_example_2']:\n    train_df[col] = train_df[col].apply(preprocessed_text)\n    test_df[col] = test_df[col].apply(preprocessed_text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Concatenate all text features for input\ndef concat_features(row):\n    return f\"{row['body']} [POS1] {row['positive_example_1']} [POS2] {row['positive_example_2']} [NEG1] {row['negative_example_1']} [NEG2] {row['negative_example_2']}\"\ntrain_df['text'] = train_df.apply(concat_features, axis=1)\ntest_df['text'] = test_df.apply(concat_features, axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prepare Sentence Transformers model\nfrom sentence_transformers import SentenceTransformer, models  \n\n# Initialize word_embedding model\nmodel_name = '/kaggle/input/roberta-base-offline/transformers/default/1/roberta-base-offline'\nword_embedding_model = models.Transformer(model_name)\npooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\nmodel = SentenceTransformer(modules=[word_embedding_model, pooling_model])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prepare training examples\nfrom sentence_transformers import InputExample  \nfrom torch.utils.data import DataLoader  \ntrain_examples = [\n    InputExample(texts=[row['text']], label=float(row['rule_violation']))\n    for _, row in train_df.iterrows()\n ]\ntrain_dataloader = DataLoader(train_examples, batch_size=16, shuffle=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Use CosineSimilarityLoss for binary classification\nfrom sentence_transformers import losses  \ntrain_loss = losses.CosineSimilarityLoss(model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Finetune the model using Sentence Transformers \nimport os\nos.environ['WANDB_MODE'] = 'disabled' \n# os.environ['WANDB_DISABLED'] = 'true'   \nos.environ['DISABLE_MLFLOW'] = 'true'  \nos.environ['COMET_DISABLE'] = '1'      \nos.environ['TENSORBOARD_DISABLE'] = 'true'  \nfrom sentence_transformers import SentenceTransformer, models, InputExample, losses\nfrom torch.utils.data import DataLoader\nmodel_name = 'roberta-base'\nword_embedding_model = models.Transformer(model_name)\npooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\nmodel = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n\n# Prepare training examples for CosineSimilarityLoss \ntrain_examples = []\nfor _, row in train_df.iterrows():\n    train_examples.append(InputExample(texts=[row['body'], row['positive_example_1']], label=1.0))\n    train_examples.append(InputExample(texts=[row['body'], row['negative_example_1']], label=0.0))\n    train_examples.append(InputExample(texts=[row['body'], row['positive_example_2']], label=1.0))\n    train_examples.append(InputExample(texts=[row['body'], row['negative_example_2']], label=0.0))\ntrain_dataloader = DataLoader(train_examples, batch_size=16, shuffle=True)\ntrain_loss = losses.CosineSimilarityLoss(model)\nmodel.fit(\n    train_objectives=[(train_dataloader, train_loss)],\n    epochs=3,\n    warmup_steps=100\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the finetuned model\nmodel.save('sentence_roberta_model')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the saved model for inference\nmodel = SentenceTransformer('sentence_roberta_model')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Inference on test set\ntest_embeddings = model.encode(test_df['text'].tolist(), batch_size=16, show_progress_bar=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Use cosine similarity to a positive class prototype\npositive_embeddings = model.encode(train_df[train_df['rule_violation'] == 1]['text'].tolist(), batch_size=16)\n\n# Use the mean embedding of positive samples as the prototype\npositive_prototype = torch.tensor(positive_embeddings).mean(dim=0)\ntest_scores = [torch.nn.functional.cosine_similarity(torch.tensor(embed), positive_prototype, dim=0).item() for embed in test_embeddings]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Format predictions for submission\nsample_submission = pd.read_csv('/kaggle/input/jigsaw-agile-community-rules/sample_submission.csv')\nmy_submission = pd.DataFrame({'row_id': test_df['row_id'], 'rule_violation': test_scores})\nmy_submission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"my_submission.head(10)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}