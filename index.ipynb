{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a8e6c28",
   "metadata": {},
   "source": [
    "# Jigsaw - Agile Community Rules Classification\n",
    "\n",
    "- Finetuning a Roberta-base transformer for binary classification to predict whether a Reddit comment violates a specific rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdd19ade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>body</th>\n",
       "      <th>rule</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>positive_example_1</th>\n",
       "      <th>positive_example_2</th>\n",
       "      <th>negative_example_1</th>\n",
       "      <th>negative_example_2</th>\n",
       "      <th>rule_violation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Banks don't want you to know this! Click here ...</td>\n",
       "      <td>No Advertising: Spam, referral links, unsolici...</td>\n",
       "      <td>Futurology</td>\n",
       "      <td>If you could tell your younger self something ...</td>\n",
       "      <td>hunt for lady for jack off in neighbourhood ht...</td>\n",
       "      <td>Watch Golden Globe Awards 2017 Live Online in ...</td>\n",
       "      <td>DOUBLE CEE x BANDS EPPS - \"BIRDS\"\\n\\nDOWNLOAD/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>SD Stream [ ENG Link 1] (http://www.sportsstre...</td>\n",
       "      <td>No Advertising: Spam, referral links, unsolici...</td>\n",
       "      <td>soccerstreams</td>\n",
       "      <td>[I wanna kiss you all over! Stunning!](http://...</td>\n",
       "      <td>LOLGA.COM is One of the First Professional Onl...</td>\n",
       "      <td>#Rapper \\nðŸš¨Straight Outta Cross Keys SC ðŸš¨YouTu...</td>\n",
       "      <td>[15 Amazing Hidden Features Of Google Search Y...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Lol. Try appealing the ban and say you won't d...</td>\n",
       "      <td>No legal advice: Do not offer or request legal...</td>\n",
       "      <td>pcmasterrace</td>\n",
       "      <td>Don't break up with him or call the cops.  If ...</td>\n",
       "      <td>It'll be dismissed: https://en.wikipedia.org/w...</td>\n",
       "      <td>Where is there a site that still works where y...</td>\n",
       "      <td>Because this statement of his is true. It isn'...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>she will come your home open her legs with  an...</td>\n",
       "      <td>No Advertising: Spam, referral links, unsolici...</td>\n",
       "      <td>sex</td>\n",
       "      <td>Selling Tyrande codes for 3â‚¬ to paypal. PM. \\n...</td>\n",
       "      <td>tight pussy watch for your cock get her at thi...</td>\n",
       "      <td>NSFW(obviously) http://spankbang.com/iy3u/vide...</td>\n",
       "      <td>Good News ::Download WhatsApp 2.16.230 APK for...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>code free tyrande ---&gt;&gt;&gt; [Imgur](http://i.imgu...</td>\n",
       "      <td>No Advertising: Spam, referral links, unsolici...</td>\n",
       "      <td>hearthstone</td>\n",
       "      <td>wow!! amazing reminds me of the old days.Well...</td>\n",
       "      <td>seek for lady for sex in around http://p77.pl/...</td>\n",
       "      <td>must be watch movie https://sites.google.com/s...</td>\n",
       "      <td>We're streaming Pokemon Veitnamese Crystal RIG...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id                                               body  \\\n",
       "0       0  Banks don't want you to know this! Click here ...   \n",
       "1       1  SD Stream [ ENG Link 1] (http://www.sportsstre...   \n",
       "2       2  Lol. Try appealing the ban and say you won't d...   \n",
       "3       3  she will come your home open her legs with  an...   \n",
       "4       4  code free tyrande --->>> [Imgur](http://i.imgu...   \n",
       "\n",
       "                                                rule      subreddit  \\\n",
       "0  No Advertising: Spam, referral links, unsolici...     Futurology   \n",
       "1  No Advertising: Spam, referral links, unsolici...  soccerstreams   \n",
       "2  No legal advice: Do not offer or request legal...   pcmasterrace   \n",
       "3  No Advertising: Spam, referral links, unsolici...            sex   \n",
       "4  No Advertising: Spam, referral links, unsolici...    hearthstone   \n",
       "\n",
       "                                  positive_example_1  \\\n",
       "0  If you could tell your younger self something ...   \n",
       "1  [I wanna kiss you all over! Stunning!](http://...   \n",
       "2  Don't break up with him or call the cops.  If ...   \n",
       "3  Selling Tyrande codes for 3â‚¬ to paypal. PM. \\n...   \n",
       "4   wow!! amazing reminds me of the old days.Well...   \n",
       "\n",
       "                                  positive_example_2  \\\n",
       "0  hunt for lady for jack off in neighbourhood ht...   \n",
       "1  LOLGA.COM is One of the First Professional Onl...   \n",
       "2  It'll be dismissed: https://en.wikipedia.org/w...   \n",
       "3  tight pussy watch for your cock get her at thi...   \n",
       "4  seek for lady for sex in around http://p77.pl/...   \n",
       "\n",
       "                                  negative_example_1  \\\n",
       "0  Watch Golden Globe Awards 2017 Live Online in ...   \n",
       "1  #Rapper \\nðŸš¨Straight Outta Cross Keys SC ðŸš¨YouTu...   \n",
       "2  Where is there a site that still works where y...   \n",
       "3  NSFW(obviously) http://spankbang.com/iy3u/vide...   \n",
       "4  must be watch movie https://sites.google.com/s...   \n",
       "\n",
       "                                  negative_example_2  rule_violation  \n",
       "0  DOUBLE CEE x BANDS EPPS - \"BIRDS\"\\n\\nDOWNLOAD/...               0  \n",
       "1  [15 Amazing Hidden Features Of Google Search Y...               0  \n",
       "2  Because this statement of his is true. It isn'...               1  \n",
       "3  Good News ::Download WhatsApp 2.16.230 APK for...               1  \n",
       "4  We're streaming Pokemon Veitnamese Crystal RIG...               1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the training dataset\n",
    "import pandas as pd\n",
    "train_df = pd.read_csv('train.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb14e2e2",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "Clean the training dataset (**_train.csv_**), remove URLs, and normalize whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afb25e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'[^\\w\\s.,!?]', '', text)  # Remove special chars except punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Normalize whitespace\n",
    "    return text\n",
    "\n",
    "for col in ['body', 'positive_example_1', 'positive_example_2', 'negative_example_1', 'negative_example_2']:\n",
    "    train_df[col] = train_df[col].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381109f1",
   "metadata": {},
   "source": [
    "## Train-Test Split using StratifiedKFold\n",
    "- Use StratifiedKFold to split the data for robust evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "310de868",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "train_idx, val_idx = next(skf.split(train_df, train_df['rule_violation']))\n",
    "train_data = train_df.iloc[train_idx]\n",
    "val_data = train_df.iloc[val_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afdb699",
   "metadata": {},
   "source": [
    "## Finetune the **RoBERTa-base** Transformer \n",
    "- Finetune the RoBERTa-base transformer for binary classification of the `rule-violation` target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8044802d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      3\u001b[39m tokenizer = RobertaTokenizer.from_pretrained(\u001b[33m'\u001b[39m\u001b[33mroberta-base\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18f4280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset for HuggingFace Trainer\n",
    "class RedditDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=128):\n",
    "        self.texts = df['body'].tolist()\n",
    "        self.labels = df['rule_violation'].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(self.texts[idx], truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "train_dataset = RedditDataset(train_data, tokenizer)\n",
    "val_dataset = RedditDataset(val_data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33926390",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    # Convert logits to probabilities\n",
    "    if logits.ndim == 2 and logits.shape[1] == 2:\n",
    "        probs = np.exp(logits) if np.all(logits >= 0) and np.all(logits <= 1) else (np.exp(logits) / np.exp(logits).sum(axis=1, keepdims=True))\n",
    "        probs = probs[:, 1]\n",
    "    else:\n",
    "        probs = logits if np.all(logits >= 0) and np.all(logits <= 1) else 1 / (1 + np.exp(-logits))\n",
    "    auc = roc_auc_score(labels, probs)\n",
    "    return {'auc': auc}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=7,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='no',\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=False,\n",
    "    metric_for_best_model='auc',\n",
    "    save_total_limit=2,\n",
    "    report_to=[]\n",
    ")\n",
    "\n",
    "best_val_auc = 0\n",
    "best_epoch = 0\n",
    "for epoch in range(1, training_args.num_train_epochs + 1):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    trainer.train()\n",
    "    # Training set AUC\n",
    "    train_preds = trainer.predict(train_dataset)\n",
    "    train_auc = train_preds.metrics.get('test_auc') or train_preds.metrics.get('auc')\n",
    "    train_loss = train_preds.metrics.get('test_loss') or train_preds.metrics.get('loss')\n",
    "    # Validation set AUC\n",
    "    val_preds = trainer.predict(val_dataset)\n",
    "    val_auc = val_preds.metrics.get('eval_auc') or val_preds.metrics.get('auc')\n",
    "    val_loss = val_preds.metrics.get('eval_loss') or val_preds.metrics.get('loss')\n",
    "    print(f\"Train Loss: {train_loss}, Train AUC: {train_auc}\")\n",
    "    print(f\"Val Loss: {val_loss}, Val AUC: {val_auc}\")\n",
    "    if val_auc and val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        best_epoch = epoch\n",
    "        model.save_pretrained('model_weights')\n",
    "        tokenizer.save_pretrained('model_weights')\n",
    "print(f\"Best Validation AUC: {best_val_auc} at epoch {best_epoch}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3203eef4",
   "metadata": {},
   "source": [
    "## Save Finetuned Model Weights and Tokenizer\n",
    "- Save model weights and tokenizer for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2d8bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('model_weights')\n",
    "tokenizer.save_pretrained('model_weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd44a57",
   "metadata": {},
   "source": [
    "## Load test.csv and sample_submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc00b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test.csv')\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa22da56",
   "metadata": {},
   "source": [
    "## Preprocess test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c6b140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the same preprocessing as train.csv\n",
    "for col in ['body', 'positive_example_1', 'positive_example_2', 'negative_example_1', 'negative_example_2']:\n",
    "    test_df[col] = test_df[col].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35e5b38",
   "metadata": {},
   "source": [
    "## Inference with Saved Model and Tokenizer\n",
    "- Load the finetuned model and tokenizer.\n",
    "- Predict `rule_violation` for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821fcd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference the finetuned RoBERTa-base transformer model\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "model = RobertaForSequenceClassification.from_pretrained('model_weights')\n",
    "tokenizer = RobertaTokenizer.from_pretrained('model_weights')\n",
    "test_texts = test_df['body'].tolist()\n",
    "inputs = tokenizer(test_texts, truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**{k: v for k, v in inputs.items()})\n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=1)[:, 1].cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1052523d",
   "metadata": {},
   "source": [
    "## Format Predictions for Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac14b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format predictions to match sample_submission.csv\n",
    "my_submission = pd.DataFrame({'row_id': test_df['row_id'], 'rule_violation': probs})\n",
    "my_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca173808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export my_submission to a CSV file\n",
    "my_submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
